{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8swFo6W94lkB"
      },
      "outputs": [],
      "source": [
        "# link of tutorial\n",
        "# https://gymnasium.farama.org/introduction/train_agent/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training an agent to play blackjack\n",
        "from collections import defaultdict\n",
        "import gymnasium as gym\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "lNW-lXgVDB_I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.core import Env\n",
        "class BlackjackAgent:\n",
        "  def __init__(\n",
        "                 self,\n",
        "                 env: gym.Env,\n",
        "                 learning_rate:float,\n",
        "                 initial_epsilon:float,\n",
        "                 epsilon_decay:float,\n",
        "                 final_epsilon:float,\n",
        "                 discount_factor:float = 0.95):\n",
        "\n",
        "    self.env = env\n",
        "\n",
        "    # deafultdict automatically creates entries with zeros for new states\n",
        "    self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "    self.lr = learning_rate\n",
        "    self.discount_factor = discount_factor\n",
        "\n",
        "    # Exploration parameters\n",
        "    self.epsilon = initial_epsilon\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.final_decay = final_epsilon\n",
        "\n",
        "    # track learning process\n",
        "    self.training_error = []\n",
        "\n",
        "  def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
        "    \"\"\"\n",
        "       obs[int, int , bool] {\n",
        "          int - player's sum of cards\n",
        "          int - dealer's showing card\n",
        "          bool - usable ace\n",
        "       }\n",
        "\n",
        "       Returns:\n",
        "             action : 0 (stand) or 1(hit)\n",
        "     \"\"\"\n",
        "    if np.random.random() < self.epsilon:\n",
        "      return self.env.action_space.sample()\n",
        "\n",
        "    else:\n",
        "      return int(np.argmax(self.q_values[obs]))\n",
        "\n",
        "\n",
        "  def update_qvalues(\n",
        "              self,\n",
        "              obs:tuple[int, int, bool],\n",
        "              action:int,\n",
        "              reward:float,\n",
        "              terminate:bool,\n",
        "              next_obs:tuple[int, int, bool]\n",
        "             ):\n",
        "    \"\"\"   \"\"\"\n",
        "    # Zero if episode reminate - no future rewards possible\n",
        "    future_q_value = (not terminate) * np.max(self.q_values[next_obs])\n",
        "\n",
        "    # target Q-value using Bellman equation\n",
        "    target = reward + self.discount_factor * future_q_value\n",
        "\n",
        "    # how wrong was our current estimators\n",
        "    temporal_difference = target - self.q_values[obs][action]\n",
        "\n",
        "    # update our estimate\n",
        "    self.q_values[obs][action] = (\n",
        "        self.q_values[obs][action] + self.lr * temporal_difference\n",
        "    )\n",
        "\n",
        "    # track learning progress\n",
        "    self.training_error.append(temporal_difference)\n",
        "\n",
        "  def decay_epsilon(self):\n",
        "    self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
      ],
      "metadata": {
        "id": "__kVq96eEvKE"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}